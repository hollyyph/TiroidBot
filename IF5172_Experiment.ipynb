{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qE6ILogixvqB"
   },
   "source": [
    "# Questions Classifier\n",
    "\n",
    "Kelompok B - IF5172 Datawarehouse & Data Mining\n",
    "- Anindya Prameswari\t\t\t13518034\n",
    "- William Fu \t\t\t\t13518055\n",
    "- Hollyana Puteri Haryono\t\t18218013\n",
    "- Adriel Gustino P. Situmorang\t\t18218047\n",
    "- Arief Purnama Muharram\t\t23521013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h_JN7C2fyjqx",
    "outputId": "fcf39d73-92af-4833-ec45-9d001f60446f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\temp\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\temp\\anaconda3\\lib\\site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\temp\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\temp\\anaconda3\\lib\\site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\temp\\anaconda3\\lib\\site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\temp\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Requirement already satisfied: gensim==3.6.0 in c:\\users\\temp\\anaconda3\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in c:\\users\\temp\\anaconda3\\lib\\site-packages (from gensim==3.6.0) (5.2.1)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\temp\\anaconda3\\lib\\site-packages (from gensim==3.6.0) (1.16.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\temp\\anaconda3\\lib\\site-packages (from gensim==3.6.0) (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\temp\\anaconda3\\lib\\site-packages (from gensim==3.6.0) (1.22.1)\n",
      "Requirement already satisfied: xgboost in c:\\users\\temp\\anaconda3\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\temp\\anaconda3\\lib\\site-packages (from xgboost) (1.8.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\temp\\anaconda3\\lib\\site-packages (from xgboost) (1.22.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade nltk\n",
    "!pip install gensim==3.6.0\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7KT_M8_rNHd",
    "outputId": "a38d0948-d127-4fe1-e236-73c2b1e93303"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TEMP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\TEMP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import necessary package\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RFAYw2E1ale"
   },
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "2nK4pJpn1Q9U",
    "outputId": "e68c78ca-7ef4-4912-ca7f-fe269b6a1f3f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Bagaimana cara mengobati penyakit Grave's?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Apa obat untuk penyakit Grave's?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Obat apa yang dapat menyembuhkan penyakit Grav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Apa gejala terkena penyakit Grave's?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Bagaimana tanda-tanda terkena penyakit Grave's?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           question\n",
       "0      5         Bagaimana cara mengobati penyakit Grave's?\n",
       "1      5                   Apa obat untuk penyakit Grave's?\n",
       "2      5  Obat apa yang dapat menyembuhkan penyakit Grav...\n",
       "3      3               Apa gejala terkena penyakit Grave's?\n",
       "4      3    Bagaimana tanda-tanda terkena penyakit Grave's?"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset2.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "Vl63uPkAuWAP",
    "outputId": "ec6eccca-3416-462f-a60b-f9e291deeea4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD1CAYAAACrz7WZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAANVElEQVR4nO3df6zd9V3H8edr7Ub44WZJL7VS2GVJ9wN/DOYVNjFm2v3AdaHEiBazpSFg/2GCP6LppgnxD5KaLCp/qEkDzKpzyBDTThIH6UQzdWyXH8qgImzUUintnc6xwQJ0vP3jfBcvl9v13vO955720+fjn3O+3+855/v+457n+d7vPefcVBWSpLa8ZtwDSJKWnnGXpAYZd0lqkHGXpAYZd0lqkHGXpAatHPcAAKtXr67JyclxjyFJJ5T777//a1U1Md+24yLuk5OTTE9Pj3sMSTqhJPnPo23ztIwkNci4S1KDjLskNci4S1KDjLskNeiYcU9ya5LDSb48a92ZSe5J8nh3uWrWto8meSLJY0neP6rBJUlHt5Aj9z8FLp2zbhuwp6rWA3u6ZZKcD2wGfqi7zx8nWbFk00qSFuSYca+qfwT+Z87qTcDO7vpO4PJZ62+rqheq6kngCeCipRlVkrRQw36IaU1VHQSoqoNJzurWnw18YdbtDnTrXiXJVmArwLnnnruonU9uu2ux8y7Kvu0bR/r4kjRqS/0H1cyzbt5/9VRVO6pqqqqmJibm/fSsJGlIw8b9UJK1AN3l4W79AeCcWbdbBzw9/HiSpGEMG/fdwJbu+hZg16z1m5OckuQ8YD3wxX4jSpIW65jn3JN8Cng3sDrJAeAGYDtwe5Krgf3AFQBV9UiS24FHgSPAtVX1nRHNLkk6imPGvaquPMqmDUe5/Y3AjX2GkiT14ydUJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGrSyz52T/BpwDVDAw8BVwGnAXwGTwD7gF6rq672mbMzktrtG+vj7tm8c6eNLOv4NfeSe5GzgOmCqqn4YWAFsBrYBe6pqPbCnW5YkLaO+p2VWAqcmWcngiP1pYBOws9u+E7i85z4kSYs0dNyr6r+AjwP7gYPAN6rqbmBNVR3sbnMQOGspBpUkLVyf0zKrGBylnwf8IHB6kg8t4v5bk0wnmZ6ZmRl2DEnSPPqclnkP8GRVzVTVS8CdwE8Ah5KsBeguD89356raUVVTVTU1MTHRYwxJ0lx94r4feGeS05IE2ADsBXYDW7rbbAF29RtRkrRYQ78VsqruS3IH8ABwBHgQ2AGcAdye5GoGLwBXLMWgkqSF6/U+96q6AbhhzuoXGBzFS5LGxE+oSlKDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNahX3JN8f5I7kvx7kr1J3pXkzCT3JHm8u1y1VMNKkham75H7TcDfVdVbgbcDe4FtwJ6qWg/s6ZYlScto6LgneT3wU8AtAFX1YlX9L7AJ2NndbCdweb8RJUmL1efI/U3ADPCJJA8muTnJ6cCaqjoI0F2etQRzSpIWoU/cVwLvAP6kqi4EnmMRp2CSbE0ynWR6ZmamxxiSpLn6xP0AcKCq7uuW72AQ+0NJ1gJ0l4fnu3NV7aiqqaqampiY6DGGJGmuoeNeVc8ATyV5S7dqA/AosBvY0q3bAuzqNaEkadFW9rz/rwCfTPI64KvAVQxeMG5PcjWwH7ii5z4kSYvUK+5V9RAwNc+mDX0eV5LUj59QlaQGGXdJapBxl6QG9f2Dqk5Ck9vuGunj79u+caSPL50MPHKXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkG+F1EnHt3LqZOCRuyQ1yCN36QTjbx5aCI/cJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBfkJV0rLx07XLxyN3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBvWOe5IVSR5M8rfd8plJ7knyeHe5qv+YkqTFWIoj9+uBvbOWtwF7qmo9sKdbliQto15xT7IO2AjcPGv1JmBnd30ncHmffUiSFq/vkfsfAr8FvDxr3ZqqOgjQXZ7Vcx+SpEUaOu5JPggcrqr7h7z/1iTTSaZnZmaGHUOSNI8+R+6XAJcl2QfcBvxMkr8ADiVZC9BdHp7vzlW1o6qmqmpqYmKixxiSpLmGjntVfbSq1lXVJLAZ+FxVfQjYDWzpbrYF2NV7SknSoozife7bgfcmeRx4b7csSVpGS/Jv9qrqXuDe7vp/AxuW4nElScPxE6qS1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNGjruSc5J8vdJ9iZ5JMn13fozk9yT5PHuctXSjStJWog+R+5HgN+oqrcB7wSuTXI+sA3YU1XrgT3dsiRpGQ0d96o6WFUPdNe/CewFzgY2ATu7m+0ELu85oyRpkZbknHuSSeBC4D5gTVUdhMELAHDWUuxDkrRwveOe5Azgr4FfrapnF3G/rUmmk0zPzMz0HUOSNEuvuCd5LYOwf7Kq7uxWH0qyttu+Fjg8332rakdVTVXV1MTERJ8xJElz9Hm3TIBbgL1V9fuzNu0GtnTXtwC7hh9PkjSMlT3uewnwYeDhJA916z4GbAduT3I1sB+4oteEkqRFGzruVfV5IEfZvGHYx5Uk9ecnVCWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhrU5z8xSdJJZXLbXSN9/H3bNy7ZY3nkLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1KCRxT3JpUkeS/JEkm2j2o8k6dVGEvckK4A/An4WOB+4Msn5o9iXJOnVRnXkfhHwRFV9tapeBG4DNo1oX5KkOVJVS/+gyc8Dl1bVNd3yh4GLq+ojs26zFdjaLb4FeGzJB/l/q4GvjfDxR835x8v5x+dEnh1GP/8bq2pivg2j+gfZmWfdK15FqmoHsGNE+3/lMMl0VU0tx75GwfnHy/nH50SeHcY7/6hOyxwAzpm1vA54ekT7kiTNMaq4fwlYn+S8JK8DNgO7R7QvSdIcIzktU1VHknwE+CywAri1qh4Zxb4WaFlO/4yQ84+X84/PiTw7jHH+kfxBVZI0Xn5CVZIaZNwlqUHGXZIa1GTck1yU5Me76+cn+fUkHxj3XMNI8pPd/O8b9ywLkeTiJK/vrp+a5HeTfCbJ7yV5w7jnW6wkfzbuGU4mSd6aZEOSM+asv3RcMy1UkuuSnHPsWy6P5v6gmuQGBt9psxK4B7gYuBd4D/DZqrpxfNMdW5IvVtVF3fVfBq4F/gZ4H/CZqto+zvmOJckjwNu7d0ztAJ4H7gA2dOt/bqwDfg9J5r5dN8BPA58DqKrLln2oJZTkqqr6xLjnOJok1zH4ed8LXABcX1W7um0PVNU7xjjeMSX5BvAc8BXgU8Cnq2pmbPM0GPeHGfxgnAI8A6yrqmeTnArcV1U/Os75jiXJg1V1YXf9S8AHqmomyenAF6rqR8Y74feWZG9Vva27/oonZJKHquqCsQ13DEkeAB4FbmbwieoweJJuBqiqfxjfdP0l2V9V5457jqPpnrvvqqpvJZlkcFDw51V10+znxfEqyYPAjzE4kPxF4DLgfgY/Q3dW1TeXc55Rff3AOB2pqu8Azyf5SlU9C1BV307y8phnW4jXJFnF4JRZvvvKX1XPJTky3tEW5MuzjhD/NclUVU0neTPw0riHO4Yp4Hrgt4HfrKqHknz7RIp6kn872iZgzXLOMoQVVfUtgKral+TdwB1J3sj8X2lyvKmqehm4G7g7yWsZnEW4Evg4MO93wIxKi3F/MclpVfU8g1dRALrzvSdC3N/A4NU+QCX5gap6pjsHeSL8gF8D3JTkdxh8YdK/JHkKeKrbdtzqnph/kOTT3eUhTrznyBrg/cDX56wP8M/LP86iPJPkgqp6CKA7gv8gcCtwXP/G2nnF87OqXmLwyfzd3ZmD5R2mwdMyp1TVC/OsXw2sraqHxzBWb0lOA9ZU1ZPjnmUhknwf8CYGcTxQVYfGPNKiJdkIXFJVHxv3LAuV5BbgE1X1+Xm2/WVV/dIYxlqQJOsY/Ob9zDzbLqmqfxrDWAuW5M1V9R/jnuO7mou7JKnRt0JK0snOuEtSg4y7JDXIuEtSg4y7JDXo/wAgw0U0le4lIQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summary\n",
    "df['label'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXo-D0Cu3Qm1"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-aIqir9mrOvO"
   },
   "outputs": [],
   "source": [
    "# read the dictionary of slang-formal words\n",
    "df_formal_words = pd.read_csv('colloquial-indonesian-lexicon.csv')\n",
    "slangs_words = df_formal_words[['slang','formal']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7Dtg6T6_sVo5"
   },
   "outputs": [],
   "source": [
    "df['question_clean'] = df['question'].apply(lambda x: re.sub(\"'\", '', x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "g8FV-aFIrJ8Q",
    "outputId": "432dd208-5961-413b-a759-7c2d2cb314e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[bagaimana, cara, mengobati, penyakit, graves, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[apa, obat, untuk, penyakit, graves, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[obat, apa, yang, dapat, menyembuhkan, penyaki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[apa, gejala, terkena, penyakit, graves, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[bagaimana, tanda-tanda, terkena, penyakit, gr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      question_clean\n",
       "0  [bagaimana, cara, mengobati, penyakit, graves, ?]\n",
       "1            [apa, obat, untuk, penyakit, graves, ?]\n",
       "2  [obat, apa, yang, dapat, menyembuhkan, penyaki...\n",
       "3        [apa, gejala, terkena, penyakit, graves, ?]\n",
       "4  [bagaimana, tanda-tanda, terkena, penyakit, gr..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA CLEANING\n",
    "# Tokenize\n",
    "df['question_clean'] = df['question_clean'].apply(nltk.word_tokenize)\n",
    "print('Tokenization complete.')\n",
    "\n",
    "df[['question_clean']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "xXACW2PJrRcw",
    "outputId": "5b7f6cb9-ce8e-4d66-a9f8-6b0cdf91878e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words removed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[mengobati, penyakit, graves, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[obat, penyakit, graves, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[obat, menyembuhkan, penyakit, graves, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[gejala, terkena, penyakit, graves, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[tanda-tanda, terkena, penyakit, graves, ?]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                question_clean\n",
       "0             [mengobati, penyakit, graves, ?]\n",
       "1                  [obat, penyakit, graves, ?]\n",
       "2    [obat, menyembuhkan, penyakit, graves, ?]\n",
       "3       [gejala, terkena, penyakit, graves, ?]\n",
       "4  [tanda-tanda, terkena, penyakit, graves, ?]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stop words\n",
    "stop_words=set(nltk.corpus.stopwords.words(\"indonesian\"))\n",
    "df['question_clean'] = df['question_clean'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "print('Stop words removed.')\n",
    "\n",
    "df[['question_clean']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "2vohm6bmrS9U",
    "outputId": "b6673ad5-d7c3-40a0-e029-1945485d8489"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers, punctuation and special characters removed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[mengobati, penyakit, graves]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[obat, penyakit, graves]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[obat, menyembuhkan, penyakit, graves]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[gejala, terkena, penyakit, graves]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[tanda-tanda, terkena, penyakit, graves]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             question_clean\n",
       "0             [mengobati, penyakit, graves]\n",
       "1                  [obat, penyakit, graves]\n",
       "2    [obat, menyembuhkan, penyakit, graves]\n",
       "3       [gejala, terkena, penyakit, graves]\n",
       "4  [tanda-tanda, terkena, penyakit, graves]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove numbers, punctuation and special characters (only keep words)\n",
    "regex = '[a-z]+'\n",
    "df['question_clean'] = df['question_clean'].apply(lambda x: [item for item in x if re.match(regex, item)])\n",
    "print('Numbers, punctuation and special characters removed.')\n",
    "\n",
    "df[['question_clean']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "LxReJt1Krw8A",
    "outputId": "1db8c31e-a840-4a39-b230-60b225d08ba2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>question</th>\n",
       "      <th>question_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Bagaimana cara mengobati penyakit Grave's?</td>\n",
       "      <td>[mengobati, penyakit, graves]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Apa obat untuk penyakit Grave's?</td>\n",
       "      <td>[obat, penyakit, graves]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Obat apa yang dapat menyembuhkan penyakit Grav...</td>\n",
       "      <td>[obat, menyembuhkan, penyakit, graves]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Apa gejala terkena penyakit Grave's?</td>\n",
       "      <td>[gejala, terkena, penyakit, graves]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Bagaimana tanda-tanda terkena penyakit Grave's?</td>\n",
       "      <td>[tanda-tanda, terkena, penyakit, graves]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           question  \\\n",
       "0      5         Bagaimana cara mengobati penyakit Grave's?   \n",
       "1      5                   Apa obat untuk penyakit Grave's?   \n",
       "2      5  Obat apa yang dapat menyembuhkan penyakit Grav...   \n",
       "3      3               Apa gejala terkena penyakit Grave's?   \n",
       "4      3    Bagaimana tanda-tanda terkena penyakit Grave's?   \n",
       "\n",
       "                             question_clean  \n",
       "0             [mengobati, penyakit, graves]  \n",
       "1                  [obat, penyakit, graves]  \n",
       "2    [obat, menyembuhkan, penyakit, graves]  \n",
       "3       [gejala, terkena, penyakit, graves]  \n",
       "4  [tanda-tanda, terkena, penyakit, graves]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YZ4bcYao1cq2"
   },
   "outputs": [],
   "source": [
    "# def preprocess(message):\n",
    "#   # Process message as follows:\n",
    "#   # 1. Remove on alphabetical symbols\n",
    "#   # 2. Case folding: turn all patterns\n",
    "#   # 3. Converts question to array of words\n",
    "#   replace_graves = re.sub(\"'\", '', message)\n",
    "#   letters_only = re.sub('[^a-zA-Z]', ' ', replace_graves)\n",
    "#   lower_case = letters_only.lower()\n",
    "#   words = np.array(lower_case.split(), dtype='object')\n",
    "#   formals = np.array([], dtype=object)  \n",
    "#   for word in words:\n",
    "#     df_tmp = slangs_words[slangs_words['slang'] == word].reset_index() \n",
    "    \n",
    "#     # Remove any informal lingo if exists\n",
    "#     if df_tmp.shape[0] != 0:\n",
    "#       formal = df_tmp['formal'][0]\n",
    "#       formals = np.append(formals, formal)\n",
    "#     else:\n",
    "#       formals = np.append(formals, word)\n",
    "    \n",
    "#     # Remove any Indonesian stopwords\n",
    "#     words_stopped = np.array([w for w in formals if w not in stopwords.words(\"indonesian\")], dtype=object)\n",
    "\n",
    "#   # return the preprocessed chat\n",
    "#   return \" \".join(words_stopped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "_6txzx9f-Wrw",
    "outputId": "3ea886b6-11d8-4dea-baca-8c7f4da15bb7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>question_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>[mengobati, penyakit, graves]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>[obat, penyakit, graves]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>[obat, menyembuhkan, penyakit, graves]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[gejala, terkena, penyakit, graves]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>[tanda-tanda, terkena, penyakit, graves]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                            question_clean\n",
       "0      5             [mengobati, penyakit, graves]\n",
       "1      5                  [obat, penyakit, graves]\n",
       "2      5    [obat, menyembuhkan, penyakit, graves]\n",
       "3      3       [gejala, terkena, penyakit, graves]\n",
       "4      3  [tanda-tanda, terkena, penyakit, graves]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prep = df.copy()\n",
    "df_prep = df_prep.drop(columns=['question'])\n",
    "df_prep.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mO21h-Z3FZEv"
   },
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sJZ4WYlCDdC_",
    "outputId": "000eee71-1db5-45c2-dcd4-916ccb1e7ab6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: (280,)\n",
      "Test dataset: (120,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "df_attr = np.array(df_prep['question_clean'])\n",
    "df_label = np.array(df_prep['label'])\n",
    "\n",
    "preprocess_train, preprocess_test, y_train, y_test = train_test_split(\n",
    "    df_attr, df_label, test_size=0.3,\n",
    "    random_state=42, stratify=df_label\n",
    ")\n",
    "\n",
    "print(f\"Train dataset: {preprocess_train.shape}\")\n",
    "print(f\"Test dataset: {preprocess_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2O8oIK1SJY7t"
   },
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2d1gGlPYinS"
   },
   "source": [
    "#### Word2vec Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_dpTGX9ig-V",
    "outputId": "ba7b2999-71e6-44fd-e292-74feb37319b4"
   },
   "outputs": [],
   "source": [
    "# # Classification using word2vec vectorizer\n",
    "\n",
    "# from gensim.models import Word2Vec\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# import xgboost as xgb\n",
    "\n",
    "# vec_model = Word2Vec(df_prep['question_clean'])\n",
    "# w2v = dict(zip(vec_model.wv.index_to_key, vec_model.wv.syn0))\n",
    "\n",
    "# class Vectorizer(object):\n",
    "    \n",
    "#     def __init__(self, vec):\n",
    "#         self.vec = vec\n",
    "#         self.dim = len(vec.values())\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         return np.array([np.mean([self.vec[w] for w in words if w in self.vec] or [np.zeros(self.dim)], axis=0) for words in X])\n",
    "\n",
    "# class Classifier(object):\n",
    "    \n",
    "#     def __init__(self, model, param):\n",
    "#         self.model = model\n",
    "#         self.param = param\n",
    "#         self.gs = GridSearchCV(self.model, self.param, cv=5, error_score=0, refit=True)        \n",
    "\n",
    "#     def fit(self, X, y):        \n",
    "#         return self.gs.fit(X, y)\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         return self.gs.predict(X)\n",
    "\n",
    "# clf_models = {\n",
    "#     'Naive Bayes': GaussianNB(),\n",
    "#     'Decision Tree': DecisionTreeClassifier(),  \n",
    "#     'Perceptron': MLPClassifier(),\n",
    "#     'Gradient Boosting': GradientBoostingClassifier(),\n",
    "#     'XGBoost': xgb.XGBClassifier()\n",
    "# }\n",
    "\n",
    "# clf_params = {\n",
    "#     'Naive Bayes': { },\n",
    "#     'Decision Tree': { 'min_samples_split': [2, 5] }, \n",
    "#     'Perceptron': { 'activation': ['tanh', 'relu'] },\n",
    "#     'Gradient Boosting': { 'learning_rate': [0.05, 0.1], 'min_samples_split': [2, 5] },\n",
    "#     'XGBoost': { \"max_depth\": [3, 4, 5], \"learning_rate\": [0.1, 0.01, 0.05], \"gamma\": [0, 0.25, 1], \"reg_lambda\": [0, 1] }\n",
    "# }\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df_prep['question_clean'], df_prep['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# for key in clf_models.keys():\n",
    "    \n",
    "#     clf = Pipeline([('Word2Vec vectorizer', Vectorizer(w2v)), ('Classifier', Classifier(clf_models[key], clf_params[key]))])\n",
    "    \n",
    "#     clf.fit(X_train, y_train)\n",
    "#     y_pred = clf.predict(X_test)\n",
    "    \n",
    "#     print(key, ':')\n",
    "#     print(\"Accuracy: %1.3f \\tPrecision: %1.3f \\tRecall: %1.3f \\t\\tF1: %1.3f\\n\" % (accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='macro'), recall_score(y_test, y_pred, average='macro'), f1_score(y_test, y_pred, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiM2k3iXHiAs"
   },
   "source": [
    "### TFIDF Vectorizer\n",
    "Dalam eksperimen ini, dimanfaatkan vectorizer TFIDF untuk mentransformasi teks menjadi vector. Vectorizer ini memiliki tambahan komponen TFIDF di atas CountVectorizer. Adapun metrik pengukuran yang dimanfaatkan dalam vectorizer ini adalah sebagai berikut.\n",
    "\n",
    "TF(t,d) + IDF(t)\n",
    "\n",
    "Keterangan:\n",
    "- *TF(t,d)*: Term Frequency, banyaknya kemunculan term t dalam dokumen d\n",
    "- *IDF(t)*: Inverse Document Frequency, ukuran yang menentukan seberapa sering sebuah term t muncul dalam corpus dokumen yang ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FbQISyS9zjLV",
    "outputId": "4cb5ea8f-f198-4e8d-8d9b-143c662321e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization complete.\n",
      "\n",
      "Naive Bayes : {'alpha': 0.5, 'fit_prior': False}\n",
      "Precision: 0.794 \tRecall: 0.799 \t\tF1: 0.773\n",
      "\n",
      "Decision Tree : {'min_samples_split': 2}\n",
      "Precision: 0.546 \tRecall: 0.504 \t\tF1: 0.514\n",
      "\n",
      "Perceptron : {'activation': 'relu', 'alpha': 0.0001}\n",
      "Precision: 0.625 \tRecall: 0.604 \t\tF1: 0.600\n",
      "\n",
      "Gradient Boosting : {'learning_rate': 0.05, 'min_samples_split': 5}\n",
      "Precision: 0.626 \tRecall: 0.580 \t\tF1: 0.587\n",
      "\n",
      "XGBoost : {'gamma': 1, 'learning_rate': 0.1, 'max_depth': 3, 'reg_lambda': 0}\n",
      "Precision: 0.681 \tRecall: 0.667 \t\tF1: 0.658\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification using TFIDF vectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "v = \"\"\n",
    "# Vectorize training and testing data\n",
    "def Vectorize(vec, X_train, X_test):    \n",
    "    \n",
    "    X_train_vec = vec.fit_transform(X_train)\n",
    "    X_test_vec = vec.transform(X_test)\n",
    "    \n",
    "    v = vec\n",
    "    print('Vectorization complete.\\n')\n",
    "    \n",
    "    return X_train_vec, X_test_vec\n",
    "\n",
    "# Use multiple classifiers and grid search for prediction\n",
    "def ML_modeling(models, params, X_train, X_test, y_train, y_test):    \n",
    "    \n",
    "    if not set(models.keys()).issubset(set(params.keys())):\n",
    "        raise ValueError('Some estimators are missing parameters')\n",
    "\n",
    "    fitted_models = {}\n",
    "    for key in models.keys():\n",
    "    \n",
    "        model = models[key]\n",
    "        param = params[key]\n",
    "        gs = GridSearchCV(model, param, cv=5, error_score=0, refit=True)\n",
    "        gs.fit(X_train, y_train)\n",
    "        y_pred = gs.predict(X_test)\n",
    "        \n",
    "        fitted_models[key] = gs\n",
    "        \n",
    "        # Print scores for the classifier\n",
    "        print(key, ':', gs.best_params_)\n",
    "        print(\"Precision: %1.3f \\tRecall: %1.3f \\t\\tF1: %1.3f\\n\" % (precision_score(y_test, y_pred, average='macro'), recall_score(y_test, y_pred, average='macro'), f1_score(y_test, y_pred, average='macro')))\n",
    "    \n",
    "    return fitted_models\n",
    "\n",
    "models = {\n",
    "    'Naive Bayes': MultinomialNB(), \n",
    "    'Decision Tree': DecisionTreeClassifier(),  \n",
    "    'Perceptron': MLPClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'XGBoost': xgb.XGBClassifier()\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'Naive Bayes': { 'alpha': [0.5, 0.75, 1], 'fit_prior': [True, False] }, \n",
    "    'Decision Tree': { 'min_samples_split': [1, 2, 5] }, \n",
    "    'Perceptron': { 'alpha': [0.0001, 0.001], 'activation': ['tanh', 'relu'] },\n",
    "    'Gradient Boosting': { 'learning_rate': [0.05, 0.1], 'min_samples_split': [2, 5] },\n",
    "    'XGBoost': { \"max_depth\": [3, 4, 5], \"learning_rate\": [0.1, 0.01, 0.05], \"gamma\": [0, 0.25, 1], \"reg_lambda\": [0, 1] }\n",
    "}\n",
    "\n",
    "# Encode label categories to numbers\n",
    "enc = LabelEncoder()\n",
    "df_prep['label'] = enc.fit_transform(df_prep['label'])\n",
    "df_prep['question_clean'] = df_prep['question_clean'].apply(lambda x: \" \".join(x)) \n",
    "labels = list(enc.classes_)\n",
    "\n",
    "# Train-test split and vectorize\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_prep['question_clean'], df_prep['label'], test_size=0.2, random_state = 42)\n",
    "X_train_vec, X_test_vec = Vectorize(TfidfVectorizer(), X_train, X_test)\n",
    "\n",
    "fitted_models = ML_modeling(models, params, X_train_vec, X_test_vec, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Naive Bayes': GridSearchCV(cv=5, error_score=0, estimator=MultinomialNB(),\n",
       "              param_grid={'alpha': [0.5, 0.75, 1], 'fit_prior': [True, False]}),\n",
       " 'Decision Tree': GridSearchCV(cv=5, error_score=0, estimator=DecisionTreeClassifier(),\n",
       "              param_grid={'min_samples_split': [1, 2, 5]}),\n",
       " 'Perceptron': GridSearchCV(cv=5, error_score=0, estimator=MLPClassifier(),\n",
       "              param_grid={'activation': ['tanh', 'relu'],\n",
       "                          'alpha': [0.0001, 0.001]}),\n",
       " 'Gradient Boosting': GridSearchCV(cv=5, error_score=0, estimator=GradientBoostingClassifier(),\n",
       "              param_grid={'learning_rate': [0.05, 0.1],\n",
       "                          'min_samples_split': [2, 5]}),\n",
       " 'XGBoost': GridSearchCV(cv=5, error_score=0,\n",
       "              estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                      callbacks=None, colsample_bylevel=None,\n",
       "                                      colsample_bynode=None,\n",
       "                                      colsample_bytree=None,\n",
       "                                      early_stopping_rounds=None,\n",
       "                                      enable_categorical=False, eval_metric=None,\n",
       "                                      gamma=None, gpu_id=None, grow_policy=None,\n",
       "                                      importance_type=None,\n",
       "                                      interaction_constraints=None,\n",
       "                                      learning_rate=None, max_b...\n",
       "                                      max_cat_to_onehot=None,\n",
       "                                      max_delta_step=None, max_depth=None,\n",
       "                                      max_leaves=None, min_child_weight=None,\n",
       "                                      missing=nan, monotone_constraints=None,\n",
       "                                      n_estimators=100, n_jobs=None,\n",
       "                                      num_parallel_tree=None, predictor=None,\n",
       "                                      random_state=None, reg_alpha=None,\n",
       "                                      reg_lambda=None, ...),\n",
       "              param_grid={'gamma': [0, 0.25, 1],\n",
       "                          'learning_rate': [0.1, 0.01, 0.05],\n",
       "                          'max_depth': [3, 4, 5], 'reg_lambda': [0, 1]})}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=5, error_score=0, estimator=MultinomialNB(),\n",
      "             param_grid={'alpha': [0.5, 0.75, 1], 'fit_prior': [True, False]})\n",
      "TfidfVectorizer()\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Model\n",
    "model_nb = fitted_models[\"Naive Bayes\"]\n",
    "# Vectorizer\n",
    "vec = TfidfVectorizer()\n",
    "X_train_vec = vec.fit_transform(X_train)\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'model_nb.sav'\n",
    "pickle.dump(model_nb, open(filename, 'wb'))\n",
    "filename_vec = 'vectorizer.sav'\n",
    "pickle.dump(vec, open(filename_vec, 'wb'))\n",
    "\n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "print(loaded_model)\n",
    "loaded_vec = pickle.load(open(filename_vec, 'rb'))\n",
    "print(loaded_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kalimat = \"apa yang menyebabkan penyakit Grave\"\n",
    "\n",
    "kalimat_vec = loaded_vec.transform([kalimat])\n",
    "pred = loaded_model.predict(kalimat_vec)\n",
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kalimat = \"definisi dari Grave\"\n",
    "\n",
    "kalimat_vec = loaded_vec.transform([kalimat])\n",
    "pred = loaded_model.predict(kalimat_vec)\n",
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kalimat = \"anjay keren bgt\"\n",
    "\n",
    "kalimat_vec = loaded_vec.transform([kalimat])\n",
    "pred = loaded_model.predict(kalimat_vec)\n",
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IF5172_Experiment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
